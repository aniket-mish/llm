{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters:  1115393\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"number of characters: \", len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1115386, 1115374, 1115385, 1115387, 1115383, 1115374, 1115385, 1115309, 1115380, 1115389, 1115373, 1115349, 1115389, 1115390, 1115391, 1115385, 1115387, 1115390, 1115385, 1115387, 1115391, 1115389]\n",
      "we are building an agi\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.stoi = { ch:i for i,ch in enumerate(vocab) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(vocab) }\n",
    "\n",
    "    def encode(self, text):\n",
    "        ids = [self.stoi[c] for c in text]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = ''.join([self.itos[i] for i in ids])\n",
    "        return text\n",
    "\n",
    "tokenizer = Tokenizer(text)\n",
    "text = \"\"\"we are building an agi\"\"\"\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[854, 553, 6282, 448, 1017, 72]\n",
      "we are building an agi\n"
     ]
    }
   ],
   "source": [
    "text = \"we are building an agi\"\n",
    "ids = enc.encode(text)\n",
    "print(ids)\n",
    "print(enc.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "\n",
    "            input_chunk = token_ids[i: i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomDataset(text, enc, max_length=4, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(dataset=data, \n",
    "                        batch_size=1,\n",
    "                        num_workers=0,\n",
    "                        drop_last=True, # drops last batch if its shorter than specified batch_size\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 854,  553, 6282,  448]]), tensor([[ 553, 6282,  448, 1017]]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "x, y = next(data_iter)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 256])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embd = embedding_layer(x)\n",
    "\n",
    "max_length=4\n",
    "pos_embedding_layer = torch.nn.Embedding(max_length, output_dim)\n",
    "pos_embd = pos_embedding_layer(torch.arange(max_length)) # 0 1 ... max_length-1\n",
    "\n",
    "input_embd = token_embd + pos_embd\n",
    "input_embd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89],\n",
    "   [0.55, 0.87, 0.66],\n",
    "   [0.57, 0.85, 0.64],\n",
    "   [0.22, 0.58, 0.33],\n",
    "   [0.77, 0.25, 0.10],\n",
    "   [0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "# step 1\n",
    "attn_scr = inputs @ inputs.T # efficient matmul\n",
    "print(attn_scr)\n",
    "\n",
    "# step 2\n",
    "attn_w = torch.softmax(attn_scr, dim=-1)\n",
    "print(attn_w)\n",
    "\n",
    "# step 3\n",
    "context_vec = attn_w @ inputs\n",
    "print(context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention mechanism with trainable matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0310, -0.5954],\n",
       "        [ 0.0372, -0.5953],\n",
       "        [ 0.0371, -0.5954],\n",
       "        [ 0.0355, -0.5953],\n",
       "        [ 0.0336, -0.5962],\n",
       "        [ 0.0367, -0.5948]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89],\n",
    "   [0.55, 0.87, 0.66],\n",
    "   [0.57, 0.85, 0.64],\n",
    "   [0.22, 0.58, 0.33],\n",
    "   [0.77, 0.25, 0.10],\n",
    "   [0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "\n",
    "        super().__init__()\n",
    "        # instead of nn.Parameter(torch.rand(d_in, d_out)) as nn.Linear has \n",
    "        # optimized weight initialization scheme and it is more efficient\n",
    "        # in matmul ops when bias=False\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        keys = self.w_k(x)\n",
    "        queries = self.w_q(x)\n",
    "        values = self.w_v(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k ** 0.5, dim=-1) # original paper d_k=64\n",
    "        context_vectors = attn_weights @ values\n",
    "\n",
    "        return context_vectors \n",
    "\n",
    "\n",
    "sa = SelfAttention(d_in, d_out)\n",
    "sa(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "causal attention: prevent model from accessing future tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5284,  0.7834],\n",
       "         [ 0.2639,  0.3914],\n",
       "         [ 0.0757,  0.4419],\n",
       "         [ 0.0026,  0.3854],\n",
       "         [-0.0873,  0.3367],\n",
       "         [-0.1483,  0.1921]],\n",
       "\n",
       "        [[ 0.5284,  0.7834],\n",
       "         [ 0.2639,  0.3914],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [-0.0531,  0.0556],\n",
       "         [-0.0637,  0.1064],\n",
       "         [-0.0464,  0.2675]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89],\n",
    "   [0.55, 0.87, 0.66],\n",
    "   [0.57, 0.85, 0.64],\n",
    "   [0.22, 0.58, 0.33],\n",
    "   [0.77, 0.25, 0.10],\n",
    "   [0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "class CasualAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, bias=False):\n",
    "\n",
    "        super().__init__()\n",
    "        # instead of nn.Parameter(torch.rand(d_in, d_out)) as nn.Linear has \n",
    "        # optimized weight initialization scheme and it is more efficient\n",
    "        # in matmul ops when bias=False\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b, tokens, d_in = x.shape\n",
    "        keys = self.w_k(x) # (2, 6, 2)\n",
    "        queries = self.w_q(x)\n",
    "        values = self.w_v(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # (2, 6, 3) @ (2, 3, 6) -> (2, 6, 6)\n",
    "\n",
    "        masked = attn_scores.masked_fill(self.mask.bool()[:tokens, :tokens], -torch.inf) # masking with -inf\n",
    "\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(masked / d_k ** 0.5, dim=-1) # original paper d_k=64\n",
    "\n",
    "        # drop by 50% and remaining are rescale by a factor of 1/0.5 -> 2\n",
    "        attn_weights = self.dropout(attn_weights) # reduce overfitting\n",
    "\n",
    "        context_vectors = attn_weights @ values # (2, 6, 6) @ (2, 6, 2)\n",
    "\n",
    "        return context_vectors # (2, 6, 2)\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_length)\n",
    "ca(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi-head attention: split attention into multiple heads where each head learns different aspect of the data and then combine the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4583, -0.7410,  0.3520,  0.9139],\n",
       "         [-0.1937, -0.3133,  0.0780, -0.0513],\n",
       "         [-0.3056, -0.2787,  0.0538, -0.0353],\n",
       "         [-0.1013, -0.1639,  0.0737, -0.0828],\n",
       "         [-0.0936, -0.1312, -0.0721, -0.0675],\n",
       "         [-0.1440, -0.1313,  0.1875,  0.1113]],\n",
       "\n",
       "        [[-0.4583, -0.7410,  0.3520,  0.9139],\n",
       "         [-0.6871, -0.7642,  0.0780, -0.0513],\n",
       "         [-0.4326, -0.4813,  0.0538, -0.0353],\n",
       "         [-0.6828, -0.6833,  0.0414, -0.0272],\n",
       "         [-0.3691, -0.3743,  0.1317,  0.1418],\n",
       "         [-0.1589, -0.1376,  0.0977,  0.0937]],\n",
       "\n",
       "        [[-0.4583, -0.7410,  0.0000,  0.0000],\n",
       "         [-0.1937, -0.3133,  0.2702,  0.4477],\n",
       "         [-0.6161, -0.5624,  0.1724,  0.3077],\n",
       "         [-0.1360, -0.1128,  0.1374,  0.1904],\n",
       "         [ 0.0000,  0.0000,  0.0259, -0.0216],\n",
       "         [-0.4578, -0.4484,  0.0282, -0.0378]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89],\n",
    "   [0.55, 0.87, 0.66],\n",
    "   [0.57, 0.85, 0.64],\n",
    "   [0.22, 0.58, 0.33],\n",
    "   [0.77, 0.25, 0.10],\n",
    "   [0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "class MultiHeadAttentionStack(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, n_head, bias=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CasualAttention(d_in, d_out, context_length) for _ in range(n_head)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "batch = torch.stack((inputs, inputs, inputs), dim=0)\n",
    "context_length = batch.shape[1] # number of tokens (3, 6, 3)\n",
    "mhas = MultiHeadAttentionStack(d_in, d_out, context_length, n_head=2)\n",
    "mhas(batch) # (3, 6, 4) -> 2 batch size, 6 number of tokens, 4 context vector size * number of heads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets combine causal attention and multi-head attention code to compute the attention in parallel. currently we're just stacking multiple causal attention blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5277, -0.4210],\n",
       "         [-0.5655, -0.2873],\n",
       "         [-0.3715, -0.3506],\n",
       "         [-0.3938, -0.2946],\n",
       "         [-0.5428, -0.2676],\n",
       "         [-0.5057, -0.4016]],\n",
       "\n",
       "        [[-0.5692, -0.4380],\n",
       "         [-0.5212, -0.2434],\n",
       "         [-0.4669, -0.4453],\n",
       "         [-0.4240, -0.3500],\n",
       "         [-0.5441, -0.3412],\n",
       "         [-0.5578, -0.3455]],\n",
       "\n",
       "        [[-0.4985, -0.3627],\n",
       "         [-0.5168, -0.3241],\n",
       "         [-0.4536, -0.3536],\n",
       "         [-0.5506, -0.2607],\n",
       "         [-0.5175, -0.4166],\n",
       "         [-0.5060, -0.3071]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89],\n",
    "   [0.55, 0.87, 0.66],\n",
    "   [0.57, 0.85, 0.64],\n",
    "   [0.22, 0.58, 0.33],\n",
    "   [0.77, 0.25, 0.10],\n",
    "   [0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, n_heads, bias=False):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # d_out must be divisible by n_heads to distribute it perfectly\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_out // n_heads\n",
    "\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b, n_tokens, d_out = x.shape\n",
    "\n",
    "        keys = self.w_k(x)\n",
    "        queries = self.w_q(x)\n",
    "        values = self.w_v(x)\n",
    "\n",
    "        # split d_out into n_heads and head_dim\n",
    "        keys = keys.view(b, n_tokens, self.n_heads, self.head_dim)\n",
    "        queries = queries.view(b, n_tokens, self.n_heads, self.head_dim)\n",
    "        values = values.view(b, n_tokens, self.n_heads, self.head_dim)\n",
    "\n",
    "        # align the matrices\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        # create mask of shape (6, 6) with true and false\n",
    "        mask = self.mask.bool()[:n_tokens, :n_tokens]\n",
    "        \n",
    "        # use mask to fill the attention scores matrix \n",
    "        attn_scores.masked_fill(mask, -torch.inf)\n",
    "        \n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k ** 0.5, dim=-1)\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, n_tokens, self.d_out)\n",
    "\n",
    "        context_vec = self.out_proj(context_vec) # linear projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "batch = torch.stack((inputs, inputs, inputs), dim=0)\n",
    "context_length = batch.shape[1] # number of tokens (3, 6, 3)\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, n_heads=2)\n",
    "mha(batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
